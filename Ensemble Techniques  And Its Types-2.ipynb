{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c92900-4747-4e5d-92e5-688885c5076a",
   "metadata": {},
   "source": [
    "# Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf37ba3-415c-4919-92e7-35a9e5e80942",
   "metadata": {},
   "source": [
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by creating multiple subsets (bootstrap samples) of the original training data, training individual decision trees on these subsets, and then averaging or voting their predictions. This process introduces randomness and diversity into the model, which helps to reduce the variance of the overall model. By combining the predictions of multiple trees, bagging reduces the tendency of individual decision trees to overfit the training data, resulting in a more robust and less overfit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06277eb-b93f-4d1f-809b-f8a1773ca653",
   "metadata": {},
   "source": [
    "# Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cf932-7f9e-475d-9574-05032ac1f417",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "Diversity: Different base learners can capture different patterns in the data, increasing diversity.\n",
    "Improved Generalization: Combining diverse base learners often leads to better generalization.\n",
    "Robustness: Ensemble methods can be more robust to noisy data.\n",
    "Disadvantages:\n",
    "\n",
    "Complexity: Using diverse base learners may increase the complexity of the ensemble.\n",
    "Training Time: Training diverse base learners can be computationally expensive.\n",
    "Compatibility: Not all base learners are suitable for bagging; some may not work well together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec8b69-1505-4006-b1a4-3d8644c6eb64",
   "metadata": {},
   "source": [
    "# Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e5aa8-bcdd-4387-b7d9-48aab0a8f016",
   "metadata": {},
   "source": [
    "If you use high-variance base learners (e.g., deep decision trees), bagging can reduce variance and improve generalization, effectively decreasing the model's overfitting tendency.\n",
    "If you use low-bias base learners (e.g., shallow decision trees or weak classifiers), bagging can help reduce bias and improve model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850c9a0-67bb-4c3c-a29d-8ae65895135d",
   "metadata": {},
   "source": [
    "# Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e41142-6871-4ba3-bda9-735c6b315527",
   "metadata": {},
   "source": [
    "For classification tasks, bagging typically involves training an ensemble of base classifiers (e.g., decision trees, random forests) and combining their predictions using majority voting.\n",
    "For regression tasks, bagging involves training an ensemble of base regressors (e.g., decision trees, random forests) and combining their predictions by averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf617f-d375-409c-91ce-3f2caa69e2dc",
   "metadata": {},
   "source": [
    "# Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e5b9f-85b7-44c1-9e3f-284b27747e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "898470c8-7fcd-461c-a453-ca6f5267b248",
   "metadata": {},
   "source": [
    "# Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaeb354-d8ab-4207-bf62-76c9bdb71ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
